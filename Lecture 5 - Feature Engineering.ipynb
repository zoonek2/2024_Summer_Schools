{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cbe2e7-cd71-42a4-bd85-a4c2d50b7d99",
   "metadata": {},
   "source": [
    "# Feature Engineering: Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c08d06-2ebd-4393-a42d-6422e4f428bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/zoonek2/2024_Summer_Schools\n",
    "! ln -s 2024_Summer_Schools/data .\n",
    "! ln -s 2024_Summer_Schools/images .\n",
    "! pip install palmerpenguins pydataset sentence_transformers timm umap-learn sentence_transformers\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22fc25aa-6c80-4482-87c3-5025e690f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import sklearn.datasets\n",
    "from pydataset import data\n",
    "from PIL import Image\n",
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d721d3-a74b-4434-958d-473381ba2aa2",
   "metadata": {},
   "source": [
    "# Financial data\n",
    "\n",
    "- Check that EPS and Price are almost proportional\n",
    "- Compute the P/E ratio\n",
    "- Plot the distribution of MCap and log(MCap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370e14a-a342-4a73-ad43-dac92428a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv( \"data/feature_analysis__financial_data.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23218c-7e86-4aed-a008-ea823a952b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a53940-3199-4715-a8a5-1ec0744b422f",
   "metadata": {},
   "source": [
    "# Images\n",
    "Compute the \"average colour\" of the following images.\n",
    "Can we use it to classify the images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876d252-7fa7-4307-9edf-6e8e88d05011",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ Image.open( f\"data/tmp{j+1}.jpg\" ) for j in range(3) ]\n",
    "images = [ np.array( image.getdata() ) for image in images ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4a4a4-fc5b-489b-bbc0-eac801affd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = [\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    for x in images\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f9ee1-9653-4ace-9aec-dd30b07cf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot the colours as follows\n",
    "fig, ax = plt.subplots( figsize = (4,1) )\n",
    "ax.scatter( [1,2,3], [0,0,0], color = [ u/255 for u in averages ], s = 2000 )\n",
    "ax.set_xlim(0,4)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4650cb2-1a86-424d-bd0d-568e661f26cd",
   "metadata": {},
   "source": [
    "# Text\n",
    "Compute some features on the following texts.\n",
    "For instance (the list is not exhaustive: add your own!)\n",
    "- average length of the words\n",
    "- average length of the sentences\n",
    "- presence of numbers\n",
    "- presence of all-caps words\n",
    "- presence of past tense\n",
    "- presence of future tense\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a25352-7452-40f9-b39a-1b5f19a74b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "kantlipsum = \"\"\"As any dedicated reader can clearly see, the Ideal of practical reason is a representation\n",
    "of, as far as I know, the things in themselves; as I have shown elsewhere, the phenomena\n",
    "should only be used as a canon for our understanding. The paralogisms of practical\n",
    "reason are what first give rise to the architectonic of practical reason. As will easily be\n",
    "shown in the next section, reason would thereby be made to contradict, in view of these\n",
    "considerations, the Ideal of practical reason, yet the manifold depends on the phenomena.\n",
    "Necessity depends on, when thus treated as the practical employment of the never-ending\n",
    "regress in the series of empirical conditions, time. Human reason depends on our sense\n",
    "perceptions, by means of analytic unity. There can be no doubt that the objects in space\n",
    "and time are what first give rise to human reason.\"\"\"\n",
    "\n",
    "trump_tweet = \"\"\"Time Magazine called to say that I was PROBABLY going to be named “Man (Person) of the Year,” like last year, but I would have to agree to an interview and a major photo shoot. I said probably is no good and took a pass. Thanks anyway!\"\"\"\n",
    "trump2 = \"\"\"So interesting to see “Progressive” Democrat Congresswomen, who originally came from countries whose governments are a complete and total catastrophe, the worst, most corrupt and inept anywhere in the world (if they even have a functioning government at all), now loudly......\"\"\"\n",
    "\n",
    "financial_news = \"\"\"Unprecedented demand for Nvidia's chips and data center services has fueled a new wave of growth for the company. With shares up over 220% in the last year, many investors probably think they've missed the boat.\"\"\"\n",
    "\n",
    "seuss = \"\"\"\n",
    "At the far end of town where the Grickle-grass grows and the wind smells slow-and-sour when it blows and no\n",
    "birds ever sing excepting old crows... is the Street of the Lifted Lorax.\n",
    "And deep in the Grickle-grass, some people say, if you look deep enough you can still see, today, where the\n",
    "Lorax once stood just as long as it could before somebody lifted the Lorax away.\n",
    "What was the Lorax? Any why was it there? And why was it lifted and taken somewhere from the far end of\n",
    "town where the Grickle-grass grows? The old Once-ler still lives here.\n",
    "Ask him, he knows.\n",
    "\"\"\"\n",
    "\n",
    "texts = [ kantlipsum, trump_tweet, trump2, financial_news, seuss ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df5beb-143a-406a-a109-ae5927a33703",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c75bc-99ea-4249-8612-36e9dfd600f8",
   "metadata": {},
   "source": [
    "# Quantitative variables\n",
    "In the following dataset, \n",
    "- Which variables would you transform with a min-max scaler?\n",
    "- Which variables would you transform with a lograrithm?\n",
    "- Are there pairs of variables you would add, subtract or divide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530fe46-d734-4880-a839-58b6b171c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = sklearn.datasets.fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "print( housing['DESCR'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b92ba8-f3d3-47c5-864a-081bbdde7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = housing['data'].copy()\n",
    "d[ housing['target_names'][0] ] = housing['target']\n",
    "#d = d[ d.columns[ np.array( [ u in [int, float] for u in d.dtypes ] ) ] ]  # Only keep the numeric variables\n",
    "d = d[['LotArea', 'TotRmsAbvGrd', '1stFlrSF', '2ndFlrSF', 'PoolArea', 'SalePrice' ]]\n",
    "d.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1e3d0-e32f-45ce-b25e-1137a55dda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde894b-ddc1-47bd-8da5-43a60793c408",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "In the following dataset: \n",
    "- Display the rows with missing values\n",
    "- Replace the missing values with the average of the column (or the most common value, for qualitative variables)\n",
    "- Can you think of another (simple, but more precise) way of imputing the missing dimensions?\n",
    "- (*) Replace the missing values with a KNN forecast from the other columns\n",
    "- (*) Compare the two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0391c30-fe22-4467-a9a5-ef35fd15aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_penguins()\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe5dac-ec5d-4cce-9fff-e84f8701be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3380a8e8-f640-42b2-9901-495cd997db3a",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "- Augment the following dataset with the square and the cube of `x`\n",
    "- (*) Forecast `y`, with a linear model, using just `x`\n",
    "- (*) Forecast `y`, with a linear model, using `x`, `x²`, `x³`\n",
    "- (*) Plot the data and the forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6044f90-a4ac-403a-8c47-52ce69bcd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "x = np.random.uniform(-1, 1, size=n) * math.pi\n",
    "y = np.sin(x) + np.random.normal(size=n) * .2\n",
    "d = pd.DataFrame( { 'x': x } )\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763ebc4-87a3-43c6-befb-6de7a24bea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274edd5-8f4f-49c7-a69e-79a40755220a",
   "metadata": {},
   "source": [
    "# Qualitative variables\n",
    "What would you do with the qualitative variables?\n",
    "- For which ones is ordinal-encoding meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c503d6b-c6d0-40f7-877f-a460024ec44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = sklearn.datasets.fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "#print( housing['DESCR'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ef184-034b-41be-adfe-bb2f26a9275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = housing['data'].copy()\n",
    "d[ housing['target_names'][0] ] = housing['target']\n",
    "d = d[['MSSubClass', 'MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'OverallQual', 'GarageQual']]\n",
    "d.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ca1d0-e84e-4acb-889e-7bf09ac38528",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae0365-12ab-4fd7-bb94-e70ef4b4a487",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "The following dataset has 784 features.\n",
    "- Reduce its dimension to 2 using PCA\n",
    "- Reduce its dimension to 2 using UMAP\n",
    "- Plot the results\n",
    "- (*) Forecast `y` from the 2 features returned by UMAP\n",
    "- (*) Forecast `y` from the 2 features returned by PCA\n",
    "- (*) Compare those models\n",
    "- (*) Forecast `y` from the first `m` principal components, for `m` ranging from 1 to 10, with a k-nearest neighbour model\n",
    "- (*) How does the performance of the model change with `m`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42897609-4939-4920-a02e-b1d9397e1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d070c-1024-48c8-a7e5-4e1bee1b35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6491351-6964-4519-bc34-9f5b20e205ad",
   "metadata": {},
   "source": [
    "# Time series\n",
    "We want to forecast future values of the following time series. \n",
    "- Would you make any transformation of the numeric data?\n",
    "- Add a few lagged values as additional features\n",
    "- Add a few rolling means as additional features\n",
    "- Add the month and the day of the year as additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01314cdb-ff6b-4ad1-aa44-c28466df832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data('AirPassengers')\n",
    "#x = pd.Series( x.iloc[:,1].values.astype(float), index = x.iloc[:,0] )\n",
    "year = np.floor( x['time'] ).astype(int)\n",
    "month = ( ( 12 * x['time'] + 1e-6 ) % 12 + 1 ).astype(int)\n",
    "date = [ f\"{u}-{v:02d}-01\" for u,v in zip( year, month ) ]\n",
    "date = pd.to_datetime(date) + datetime.timedelta(35)\n",
    "date = [ str(u)[:7] + \"-01\" for u in date ]\n",
    "date = pd.to_datetime( date ) - datetime.timedelta(1)\n",
    "d = pd.DataFrame( {\n",
    "    'date': date,\n",
    "    'AirPassengers': x.iloc[:,1].values.astype(float),\n",
    "} )\n",
    "d.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c38a50-a87b-49fe-9a55-5032f8c2b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04e1f7-6a5a-41f4-8d8c-dd09d441527e",
   "metadata": {},
   "source": [
    "# Text: term-document matrix\n",
    "- What are the most common words in those texts? Try to remove words common in all of them.\n",
    "- Compute the TF-IDF term-document matrix\n",
    "- Reduce its dimension; plot the result\n",
    "- (*) Same question with the *paragraphs* in those texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73020b5-b146-4a02-8f05-a0b18601ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
    "!wget -nc https://www.gutenberg.org/cache/epub/11/pg11.txt\n",
    "!wget -nc https://www.gutenberg.org/cache/epub/1513/pg1513.txt\n",
    "!wget -nc https://www.gutenberg.org/cache/epub/98/pg98.txt\n",
    "!wget -nc https://www.gutenberg.org/cache/epub/67098/pg67098.txt\n",
    "\n",
    "titles = {\n",
    "    64317: \"The Great Gatsby\",\n",
    "    11:    \"Alice's Adventures in Wonderland\",\n",
    "    1513:  \"Romeo and Juliet\",\n",
    "    98:    \"A tale of two cities\",\n",
    "    67098: \"Winnie the Pooh\",\n",
    "}\n",
    "\n",
    "texts = {}\n",
    "for id, title in titles.items():\n",
    "    f = open( f\"pg{id}.txt\", \"r\" )\n",
    "    texts[title] = f.read()\n",
    "    f.close()\n",
    "\n",
    "def remove_legalese(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    i1 = np.argwhere( np.array( [ u.startswith( '*** START' ) for u in text ] ) )[0,0]\n",
    "    i2 = np.argwhere( np.array( [ u.startswith( '*** END' ) for u in text ] ) )[0,0]\n",
    "    text = text[i1+1:i2]\n",
    "    text = '\\n'.join(text)\n",
    "    return text\n",
    "    \n",
    "texts = { title: remove_legalese(text) for title, text in texts.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155067be-c8d2-4a0c-a257-0390ced9013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c05c0c-c201-43a8-a0b6-d067afe874bd",
   "metadata": {},
   "source": [
    "# Constant features\n",
    "In the following dataset:\n",
    "- Are there constant features?\n",
    "- Are there nearly constant features?\n",
    "- Are there qualitative features with no repeated values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f863d-7b35-4dc1-bcc3-74c276f74222",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = sklearn.datasets.fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "#print( housing['DESCR'] )\n",
    "d = housing['data'].copy()\n",
    "d[ housing['target_names'][0] ] = housing['target']\n",
    "d = d[['Id', 'LotShape', 'Utilities', 'OverallQual', 'CentralAir', 'PoolQC']]\n",
    "d.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16f281-fa41-428b-8a59-c5d2f58b9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa73a5-1635-4dc3-8542-386d44dbceaf",
   "metadata": {},
   "source": [
    "# Redundant features\n",
    "In the following dataset: \n",
    "- What are the 2 features with the largest correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e039f05-a440-4535-a78b-7cf102a3051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = sklearn.datasets.fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "#print( housing['DESCR'] )\n",
    "d = housing['data'].copy()\n",
    "d[ housing['target_names'][0] ] = housing['target']\n",
    "i = np.array([ u in [int, float] for u in d.dtypes ])\n",
    "d = d.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837969e-8d36-4ffc-a2fc-6767fa890abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array([ u in [int, float] for u in d.dtypes ])\n",
    "d = d.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c44d4-e1b1-4e16-bc0f-bd23a8640f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdca64f3-bab3-4b7f-a9cf-928a3ba86817",
   "metadata": {},
   "source": [
    "# Useful features\n",
    "- What are the 12 predictors most correlated with the target variable “SalePrice”?\r",
    "- \n",
    "Plot a scatterplot of SalePrice against each of those variable\n",
    "- (*) Fit linear models with increasingly more of those variables\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d524c8-2f03-4512-9663-66c5cb5c127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = sklearn.datasets.fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "#print( housing['DESCR'] )\n",
    "d = housing['data'].copy()\n",
    "d[ housing['target_names'][0] ] = housing['target']\n",
    "i = np.array([ u in [int, float] for u in d.dtypes ])\n",
    "d = d.iloc[:,i]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d2e5f-b04a-452d-853e-141e22638683",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc79244-30f4-4316-90f1-cf5deed7d5e9",
   "metadata": {},
   "source": [
    "# Image embeddings (*)\n",
    "\n",
    "The following code computes a 4096-dimensional embedding of the *.jpg files in the `images` directory.\n",
    "- Reduce the dimension to 2 and plot the data. You can use a scatter plot, or use the images themselves (with `plt.imshow`, which has an `extent` argument).\n",
    "- Can you think of a simple model to distinguish cat pictures from dog pictures (not one of the models you saw yesterday, something even simpler)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9bcda-6ee7-451d-8d09-593996a007eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# Model to compute image embeddings\n",
    "model = timm.create_model( 'vgg19.tv_in1k', pretrained=True, num_classes=0 )\n",
    "model = model.eval()\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "# List of images (feel free to add more, or use your own)\n",
    "duplicates = set( [ 'tmp3.jpg', 'tmp2.jpg', 'tmp1.jpg', 'i07.jpg', 'text1.jpg' ] )\n",
    "files = os.listdir(\"images\")\n",
    "files = [ f for f in files if f not in duplicates ]\n",
    "files = [ f for f in files if f.endswith(\".jpg\") ]\n",
    "files = [ f\"images/{f}\" for f in files ]\n",
    "np.random.shuffle( files )\n",
    "embeddings = {}\n",
    "\n",
    "def read_image(file): \n",
    "    \"\"\"\n",
    "    Copilot (DALL-E) does not allow me to choose the aspect ratio.\n",
    "    It used to generate only square images, but it now only produces 7×4 images.\n",
    "    Crop the images, if needed.\n",
    "    \"\"\"\n",
    "    img = Image.open(file)\n",
    "    w, h = img.size\n",
    "    assert w >= h, f\"I expect the images to have a square or landscape aspect ratio: {file} has a portrait aspect ratio, {w}×{h}\"\n",
    "    a = ( w - h ) // 2\n",
    "    img = img.crop( (a, 0, a+h, h) )\n",
    "    return img    \n",
    "\n",
    "# Compute the embeddings\n",
    "for file in tqdm(files): \n",
    "    img = read_image(file)\n",
    "    output = model(transforms(img).unsqueeze(0))\n",
    "    embeddings[ file ] = output.detach().numpy().flatten()\n",
    "embeddings = pd.DataFrame( embeddings ).T    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074cfb5-3f83-4068-ac37-64998b98a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef74952-d6a2-42fd-b59f-0b6bfa7d7169",
   "metadata": {},
   "source": [
    "# Word embeddings (*)\n",
    "\n",
    "Here are vector embeddings of 400,000 words. \n",
    "- Reduce their dimension to 2 and plot the resulting cloud of points\n",
    "- Use the embeddings to find 100 names of animals (hint: pick an animal, and take the 100 closest words)\n",
    "- Perform the dimension reduction on those 100 words alone, and plot the corresponding cloud of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51a1424-8194-4070-a2b5-0cbb650bedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "#vectors = gensim.downloader.load('glove-twitter-25')\n",
    "vectors = gensim.downloader.load('glove-wiki-gigaword-300')  # SLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44624423-83c7-4f1d-b310-91d4aa1d0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9794c7-2132-4e0a-b5d8-c4f6061c163a",
   "metadata": {},
   "source": [
    "# Sentence embeddings (*) **MAY NOT WORK ON GOOGLE COLAB**\n",
    "Here are embeddings of sentences from a few novels.\n",
    "- Reduce the dimension of the data to 2, and plot the corresponding cloud of points\n",
    "- Are the sentences in those novels homogeneous, or do you see clusters for some of them? Can you interpret those clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ff1c08-dadf-42d5-9042-692cfb5200ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model)\n",
    "\n",
    "ids = {\n",
    "    'The Great Gatsby':                 64317,\n",
    "    \"Alice's Adventures in Wonderland\":    11,\n",
    "    'Romeo and Juliet':                  1513,\n",
    "    'A tale of two cities':                98,\n",
    "    'Winnie the Pooh':                  67098,\n",
    "}\n",
    "\n",
    "def remove_legalese(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    i1 = np.argwhere( np.array( [ u.startswith( '*** START' ) for u in text ] ) )[0,0]\n",
    "    i2 = np.argwhere( np.array( [ u.startswith( '*** END' ) for u in text ] ) )[0,0]\n",
    "    text = text[i1+1:i2]\n",
    "    text = '\\n'.join(text)\n",
    "    return text\n",
    "\n",
    "raw_texts = {}\n",
    "for title, id in ids.items():\n",
    "    with open( f\"data/pg{id}.txt\" ) as f:\n",
    "        raw_texts[ title ] = f.read()       \n",
    "\n",
    "texts = { title: remove_legalese( text ) for title, text in raw_texts.items() }\n",
    "sentences = { \n",
    "    title: nltk.sent_tokenize( text )\n",
    "    for title, text in texts.items() \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c57a808c-84fc-4e99-95c0-6d69c00002f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = { title: model.encode( novel ) for title, novel in sentences.items() }  # 3 minutes locally, much more (30?) on Google Colab\n",
    "embeddings = { title: pd.DataFrame( e, index = sentences[title] ) for title, e in embeddings.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d0ced-3f03-4007-8d28-aa839e9ff06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "constrained_regression",
   "language": "python",
   "name": "constrained_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
